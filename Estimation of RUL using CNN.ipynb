{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def windows(nrows, size):\n",
    "    start,step = 0, 2\n",
    "    while start < nrows:\n",
    "        yield start, start + size\n",
    "        start += step\n",
    "\n",
    "def segment_signal(features,labels,window_size = 15):\n",
    "    segments = np.empty((0,window_size))\n",
    "    segment_labels = np.empty((0))\n",
    "    nrows = len(features)\n",
    "    for (start, end) in windows(nrows,window_size):\n",
    "        if(len(data.iloc[start:end]) == window_size):\n",
    "            segment = features[start:end].T  #Transpose to get segment of size 24 x 15 \n",
    "            label = labels[(end-1)]\n",
    "            segments = np.vstack([segments,segment]) \n",
    "            segment_labels = np.append(segment_labels,label)\n",
    "    segments = segments.reshape(-1,24,window_size,1) # number of features  = 24 \n",
    "    segment_labels = segment_labels.reshape(-1,1)\n",
    "    return segments,segment_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"PHM08.csv\")\n",
    "features = scale(data.iloc[:,2:26]) # select required columns and scale them\n",
    "labels = data.iloc[:,26] # select RUL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "segments, labels = segment_signal(features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_split = np.random.rand(len(segments)) < 0.70\n",
    "train_x = segments[train_test_split]\n",
    "train_y = labels[train_test_split]\n",
    "test_x = segments[~train_test_split]\n",
    "test_y = labels[~train_test_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(1.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def apply_conv(x,kernel_height,kernel_width,num_channels,depth):\n",
    "    weights = weight_variable([kernel_height, kernel_width, num_channels, depth])\n",
    "    biases = bias_variable([depth])\n",
    "    return tf.nn.relu(tf.add(tf.nn.conv2d(x, weights,[1,1,1,1],padding=\"VALID\"),biases))\n",
    "    \n",
    "def apply_max_pool(x,kernel_height,kernel_width,stride_size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, kernel_height, kernel_width, 1], strides=[1, 1, stride_size, 1], padding = \"VALID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_labels = 1\n",
    "batch_size = 10\n",
    "num_hidden = 800\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 30\n",
    "input_height = 24\n",
    "input_width = 15\n",
    "num_channels = 1\n",
    "total_batches = train_x.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_conv(X, kernel_height = 24, kernel_width = 4, num_channels = 1, depth = 8) \n",
    "p = apply_max_pool(c,kernel_height = 1, kernel_width = 2, stride_size = 2) \n",
    "c = apply_conv(p, kernel_height = 1, kernel_width = 3, num_channels = 8, depth = 14) \n",
    "p = apply_max_pool(c,kernel_height = 1, kernel_width = 2, stride_size = 2) \n",
    "\n",
    "shape = p.get_shape().as_list()\n",
    "flat = tf.reshape(p, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights = weight_variable([shape[1] * shape[2] * shape[3], num_hidden])\n",
    "f_biases = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(flat, f_weights),f_biases))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.add(tf.matmul(f, out_weights),out_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost_function = tf.reduce_mean(tf.square(y_- Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set MSE\n",
      "4752.72697916\n",
      "4677.79731598\n",
      "4675.55796569\n",
      "4668.99871858\n",
      "4646.43073364\n",
      "4606.30021167\n",
      "4544.63535847\n",
      "4453.55062054\n",
      "4320.07571005\n",
      "4142.02065626\n",
      "3939.75333712\n",
      "3732.80059479\n",
      "3540.9024371\n",
      "3373.17483584\n",
      "3231.20530547\n",
      "3112.15695777\n",
      "3011.5041224\n",
      "2925.24642685\n",
      "2851.78353692\n",
      "2789.1534664\n",
      "2736.44654112\n",
      "2687.96121346\n",
      "2644.67107264\n",
      "2605.52678315\n",
      "2568.49537431\n",
      "2533.52371995\n",
      "2503.20061828\n",
      "2473.34282306\n",
      "2445.49239087\n",
      "2419.93309399\n",
      "Test set MSE: 2607.9456\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Training set MSE\")\n",
    "    for epoch in range(training_epochs):\n",
    "        for b in range(total_batches):    \n",
    "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size),:]\n",
    "            _, c = session.run([optimizer, cost_function],feed_dict={X: batch_x, Y : batch_y})\n",
    "            \n",
    "        p_tr = session.run(y_, feed_dict={X:  train_x})\n",
    "        tr_mse = tf.reduce_mean(tf.square(p_tr - train_y))\n",
    "        print(session.run(tr_mse))\n",
    "\n",
    "    p_ts = session.run(y_, feed_dict={X:  test_x})\n",
    "    ts_mse = tf.reduce_mean(tf.square(p_ts - test_y))\n",
    "    print(\"Test set MSE: %.4f\" % session.run(ts_mse)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
